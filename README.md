# nyc-yellow-trip-pipeline

![Python](https://img.shields.io/badge/Python-3.10-blue)
![Spark](https://img.shields.io/badge/Apache%20Spark-Dataframe-orange)
![Databricks](https://img.shields.io/badge/Databricks-Platform-red)
![DeltaLake](https://img.shields.io/badge/Delta%20Lake-Acid%20Tables-green)

## üìå Vis√£o Geral

Este projeto implementa um pipeline de dados completo utilizando o Databricks e o formato Delta Lake para processar e analisar dados de corridas de t√°xis amarelos (Yellow Taxi) da cidade de Nova York no ano de 2025.

A arquitetura do projeto segue o padr√£o de camadas do Medallion Architecture:

- **Bronze**: ingest√£o bruta de dados
- **Silver**: limpeza, tratamento e enriquecimento
- **Gold**: agrega√ß√µes e tabelas anal√≠ticas

O objetivo √© demonstrar boas pr√°ticas de engenharia de dados com automa√ß√£o, padroniza√ß√£o e gera√ß√£o de insights.

---

## üß± Estrutura do Projeto

```
nyc-yellow-trip-pipeline/
‚îÇ
‚îú‚îÄ‚îÄ 01_bronze_ingestion/
‚îÇ   ‚îî‚îÄ‚îÄ ingestion_2025.py
‚îÇ
‚îú‚îÄ‚îÄ 02_silver_transformation/
‚îÇ   ‚îî‚îÄ‚îÄ cleaning_transformation.py
‚îÇ
‚îú‚îÄ‚îÄ 03_gold_analysis/
‚îÇ   ‚îú‚îÄ‚îÄ analytical_tables.py
‚îÇ   ‚îî‚îÄ‚îÄ docs/
‚îÇ       ‚îú‚îÄ‚îÄ monthly_revenue_table.png
‚îÇ       ‚îú‚îÄ‚îÄ payment_method_table.png
‚îÇ       ‚îî‚îÄ‚îÄ revenue_by_day_table.png
‚îÇ
‚îú‚îÄ‚îÄ pipeline_en.py
‚îî‚îÄ‚îÄ README.md
```

---

## üß™ Tecnologias Utilizadas

- **Databricks**
- **PySpark**
- **Delta Lake**
- **Python**
- **Lakehouse (Unity Catalog)**
- **Cloud Storage (via dbutils.fs.cp)**

---

## ‚öôÔ∏è Etapas do Pipeline

### 1. Camada Bronze ‚Äì Ingest√£o Bruta
- Download dos arquivos mensais (`yellow_tripdata_2025-MM.parquet`) via URL p√∫blica.
- L√≥gica de retry para garantir robustez.
- Salvamento em volume `bronze`.

### 2. Camada Silver ‚Äì Limpeza e Enriquecimento
- Leitura da tabela bronze.
- Limpeza de dados:
  - Padroniza√ß√£o para `snake_case`
  - Remo√ß√£o de registros inv√°lidos ou nulos
  - Elimina√ß√£o de duplicatas
- Enriquecimento com:
  - Nomes descritivos dos vendors
  - Tipos de pagamento detalhados
- Resultado salvo em `lakehouse.silver.yellow_trip`.

### 3. Camada Gold ‚Äì Tabelas Anal√≠ticas
- Gera√ß√£o de m√∫ltiplas tabelas agregadas em Delta:
  - **Faturamento mensal por empresa**
  - **Corridas por forma de pagamento**
  - **Total faturado por vendor**
  - **Dias mais lucrativos**
  - **Hor√°rios de maior faturamento**

---

## üìä Insights Gerados

- **Empresas mais lucrativas**
- **Dias com maior volume e rentabilidade**
- **Distribui√ß√£o por forma de pagamento**
- **Hor√°rios mais rent√°veis**
- **Comparativo de dist√¢ncia m√©dia x faturamento**

---

### üß† Descri√ß√£o das Decis√µes Tomadas

1. **Arquitetura em Camadas (Medallion Architecture):**
   - Foi adotado o modelo de camadas **Bronze ‚Üí Silver ‚Üí Gold** para garantir a organiza√ß√£o do pipeline, facilitar a manuten√ß√£o e permitir reaproveitamento dos dados em diferentes n√≠veis de qualidade.

2. **Formato Delta Lake:**
   - Todas as tabelas foram salvas como Delta Tables para garantir performance, versionamento, controle de esquema e opera√ß√µes ACID.

3. **Transforma√ß√µes com PySpark:**
   - Todo o tratamento de dados foi feito com PySpark, aproveitando o ambiente distribu√≠do do Databricks para maior escalabilidade.

4. **Padroniza√ß√£o das Colunas:**
   - Os nomes das colunas foram convertidos para o formato `snake_case` por quest√µes de legibilidade e boas pr√°ticas de engenharia de dados.

5. **Enriquecimento dos Dados:**
   - Foram adicionadas colunas descritivas para identificar:
     - **Nome das empresas (vendors)** a partir do `vendor_id`
     - **Tipo de pagamento** a partir do `payment_type`

6. **Valida√ß√£o dos Dados:**
   - Registros inv√°lidos foram removidos com filtros simples como:
     - `passenger_count > 0`
     - `trip_distance > 0`
     - `fare_amount > 0`
   - Tamb√©m foram removidas duplicatas com `.dropDuplicates()`.

7. **Orquestra√ß√£o Automatizada:**
   - Criado um notebook orquestrador para executar as tr√™s etapas em ordem (Bronze ‚Üí Silver ‚Üí Gold), com tratamento de erro em cada etapa (`try/except`).

---


Claro! Aqui est√° a mesma se√ß√£o reescrita na **primeira pessoa**, como se voc√™ mesmo estivesse alertando outras pessoas que forem usar ou manter seu projeto:

---

### ‚ö†Ô∏è Aten√ß√£o: Problemas com Incompatibilidade de Tipos em Arquivos Parquet

Durante o desenvolvimento deste projeto, enfrentei erros ao tentar ler ou gravar m√∫ltiplos arquivos Parquet com o Spark. A causa foi a **incompatibilidade de tipos entre os arquivos**, mesmo quando os nomes e estruturas pareciam semelhantes.

Por exemplo, ao tentar fazer `append` em tabelas ou unir arquivos como `yellow_tripdata_2023-01.parquet` e `yellow_tripdata_2023-02.parquet`, o Spark apresentou erros como:

```
Expected Spark type: double
Actual Parquet type: int
```

Ou:

```
Expected Spark type: StringType
Actual Parquet type: INT32
```

Esses erros ocorreram porque colunas como:

* `RatecodeID`
* `payment_type`
* `PULocationID`, `DOLocationID`
* `store_and_fwd_flag`

tinham tipos diferentes entre os arquivos ‚Äî `integer` em um e `double` ou `long` em outro, por exemplo.

---


## üë®‚Äçüíª Autor

> Projeto desenvolvido como exerc√≠cio de Engenharia de Dados, com foco em boas pr√°ticas, organiza√ß√£o e padroniza√ß√£o. Finalidade educacional e demonstrativa.
