{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b5cdab1-9e05-4ec7-852c-668eeadb3fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Camada Bronze – Ingestão de Dados\n",
    "\n",
    "##  Documentação\n",
    "\n",
    "**Objetivo:**  \n",
    "Realizar a ingestão bruta dos arquivos `yellow_tripdata_2025-MM.parquet` diretamente de uma URL pública, com lógica de retry e salvamento no volume bronze.\n",
    "\n",
    "**Entradas:**  \n",
    "- URL pública com os arquivos de viagens mensais em formato Parquet.\n",
    "\n",
    "**Saídas:**  \n",
    "- Arquivos salvos na camada `bronze` (`/Volumes/lakehouse/bronze/filestore/`).\n",
    "\n",
    "**Transformações aplicadas:**  \n",
    "- Nenhuma transformação nos dados (cópia bruta).\n",
    "- Retry automático caso falhe por erro HTTP.\n",
    "- Skip do mês caso o arquivo não seja encontrado (erro 403).\n",
    "\n",
    "##  Decisões Tomadas\n",
    "\n",
    "- Adotado o volume Delta para garantir estruturação futura.\n",
    "- Retry configurado com 5 tentativas e 5s de espera.\n",
    "- Controle de erro simples para permitir automação no pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c33b48c5-b01c-4290-a976-203e08fb022c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importações"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas\n",
    "import pyspark.sql.functions as F\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74a4b9d-4e29-40ef-bb34-c01b3c6d3212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"vendorid\", StringType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", StringType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", StringType(), True),\n",
    "    StructField(\"passenger_count\", StringType(), True),\n",
    "    StructField(\"trip_distance\", StringType(), True),\n",
    "    StructField(\"ratecodeid\", StringType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"pulocationid\", StringType(), True),\n",
    "    StructField(\"dolocationid\", StringType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", StringType(), True),\n",
    "    StructField(\"extra\", StringType(), True),\n",
    "    StructField(\"mta_tax\", StringType(), True),\n",
    "    StructField(\"tip_amount\", StringType(), True),\n",
    "    StructField(\"tolls_amount\", StringType(), True),\n",
    "    StructField(\"improvement_surcharge\", StringType(), True),\n",
    "    StructField(\"total_amount\", StringType(), True),\n",
    "    StructField(\"congestion_surcharge\", StringType(), True),\n",
    "    StructField(\"airport_fee\", StringType(), True)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1793114-2c2b-4f9f-b598-8e8b85c683c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pegando valores de variáveis"
    }
   },
   "outputs": [],
   "source": [
    "months_to_ingest = 3 # int( dbutils.widgets.get(\"months_to_ingest\"))\n",
    "year_to_ingest =2023 #dbutils.widgets.get(\"year_to_ingest\")\n",
    "\n",
    "if months_to_ingest > 12:\n",
    "    months_to_ingest = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a038fc47-2cea-4be3-bf20-0bdb85a0349c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingestão de Dados Mensais com Re-tentativas"
    }
   },
   "outputs": [],
   "source": [
    "url_base = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n",
    "bronze_path = f\"/Volumes/lakehouse/bronze/filestore/{year_to_ingest}/\"\n",
    "dbutils.fs.mkdirs(bronze_path)\n",
    "\n",
    "max_retries = 5\n",
    "retry_delay = 5  # seconds\n",
    "\n",
    "for month in range(1, months_to_ingest + 1):\n",
    "    try:\n",
    "        month_str = f\"{month:02d}\"\n",
    "        file_date = f\"yellow_tripdata_{year_to_ingest}-{month_str}.parquet\"\n",
    "        url = url_base + file_date\n",
    "        retries = 0\n",
    "\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                # dbutils.fs.cp(url, bronze_path)\n",
    "                df = spark.read.parquet(bronze_path + file_date)\n",
    "                df = df.withColumn(\"date_processing\", F.current_timestamp())\n",
    "                df.write.mode(\"append\").saveAsTable(\"lakehouse.bronze.yellow_trip\")\n",
    "                  # try to copy the file\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"403\" in str(e):  # file not found\n",
    "                    print(f\"File not found: {file_date}\")\n",
    "                    break\n",
    "                retries += 1\n",
    "                if retries >= max_retries:\n",
    "                    raise e\n",
    "                time.sleep(retry_delay)  # wait before retrying\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying {file_date}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab5e8d8-9e5d-471a-a027-0da0d7f29c30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Escrita dos dados na bronze"
    }
   },
   "source": [
    "bronze_path = f\"/Volumes/lakehouse/bronze/filestore/2023/\"\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"vendorid\", StringType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", StringType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", StringType(), True),\n",
    "    StructField(\"passenger_count\", StringType(), True),\n",
    "    StructField(\"trip_distance\", StringType(), True),\n",
    "    StructField(\"ratecodeid\", StringType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"pulocationid\", StringType(), True),\n",
    "    StructField(\"dolocationid\", StringType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", StringType(), True),\n",
    "    StructField(\"extra\", StringType(), True),\n",
    "    StructField(\"mta_tax\", StringType(), True),\n",
    "    StructField(\"tip_amount\", StringType(), True),\n",
    "    StructField(\"tolls_amount\", StringType(), True),\n",
    "    StructField(\"improvement_surcharge\", StringType(), True),\n",
    "    StructField(\"total_amount\", StringType(), True),\n",
    "    StructField(\"congestion_surcharge\", StringType(), True),\n",
    "    StructField(\"airport_fee\", StringType(), True)\n",
    "\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).parquet(bronze_path)\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"lakehouse.bronze.yellow_trip\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_data_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
