{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5cdab1-9e05-4ec7-852c-668eeadb3fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Camada Bronze – Ingestão de Dados\n",
    "\n",
    "##  Documentação\n",
    "\n",
    "**Objetivo:**  \n",
    "Realizar a ingestão bruta dos arquivos `yellow_tripdata_2025-MM.parquet` diretamente de uma URL pública, com lógica de retry e salvamento no volume bronze.\n",
    "\n",
    "**Entradas:**  \n",
    "- URL pública com os arquivos de viagens mensais em formato Parquet.\n",
    "\n",
    "**Saídas:**  \n",
    "- Arquivos salvos na camada `bronze` (`/Volumes/lakehouse/bronze/filestore/`).\n",
    "\n",
    "**Transformações aplicadas:**  \n",
    "- Nenhuma transformação nos dados (cópia bruta).\n",
    "- Retry automático caso falhe por erro HTTP.\n",
    "- Skip do mês caso o arquivo não seja encontrado (erro 403).\n",
    "\n",
    "##  Decisões Tomadas\n",
    "\n",
    "- Adotado o volume Delta para garantir estruturação futura.\n",
    "- Retry configurado com 5 tentativas e 5s de espera.\n",
    "- Controle de erro simples para permitir automação no pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c33b48c5-b01c-4290-a976-203e08fb022c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importações"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70be6c55-e37b-4a68-93b5-b2cb302722e8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configurações iniciais e Parâmetros"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CONFIGURAÇÕES\n",
    "url_base = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n",
    "bronze_root = \"/Volumes/lakehouse/bronze/filestore/\"\n",
    "\n",
    "#exponential backoff - usados para lidar com falhas temporárias no download (resiliência).\n",
    "max_retries = 5\n",
    "retry_delay = 5  # segundos\n",
    "\n",
    "# >> Entrada configurável\n",
    "year_to_ingest = 2023\n",
    "months_to_ingest = 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f20cf3-cc35-4dcd-a560-373478eda0e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validações e criação da Pasta"
    }
   },
   "outputs": [],
   "source": [
    "# >>> Validações\n",
    "ano_atual = datetime.now().year\n",
    "\n",
    "if not (1 <= months_to_ingest <= 12):\n",
    "    raise ValueError(\"Mês inválido! Informe um número entre 1 e 12.\")\n",
    "if year_to_ingest > ano_atual:\n",
    "    raise ValueError(f\"Ano inválido! {year_to_ingest} é maior que o ano atual ({ano_atual})\")\n",
    "\n",
    "# >>> Caminho do ano\n",
    "bronze_path = f\"{bronze_root}{year_to_ingest}/\"\n",
    "dbutils.fs.mkdirs(bronze_path)  # cria pasta se não existir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f27073-1c7f-4fa3-9363-45328f6e96b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Loop de ingestão de arquivos"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# >>> Download dos arquivos parquet mês a mês\n",
    "# Implementa lógica de retries com time.sleep para aguardar antes de tentar novamente.\n",
    "# O try/except externo evita que o processo pare caso um mês falhe — o loop continua para os demais.\n",
    "\n",
    "for month in range(1, months_to_ingest + 1):\n",
    "    try:\n",
    "        month_str = f\"{month:02d}\"\n",
    "        file_date = f\"yellow_tripdata_{year_to_ingest}-{month_str}.parquet\"\n",
    "        url = url_base + file_date\n",
    "        retries = 0\n",
    "\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                dbutils.fs.cp(url, bronze_path) #dbutils.fs.cp para copiar o arquivo .parquet da URL para a pasta bronze.\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"403\" in str(e): # Se erro 403 for detectado (arquivo não disponível), ele interrompe a tentativa daquele mês.\n",
    "                    break\n",
    "                retries += 1\n",
    "                if retries >= max_retries:\n",
    "                    raise e\n",
    "                time.sleep(retry_delay)\n",
    "\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "640463e3-8a3e-4c33-8ed8-0a363d20e288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# >>> Leitura e gravação na tabela Delta\n",
    "    # - dbutils.fs.ls() para listar os arquivos no diretório da camada Bronze.\n",
    "    # - try/except permite capturar problemas como caminho inválido, falta de permissões, etc.\n",
    "\n",
    "try:\n",
    "    arquivos = dbutils.fs.ls(bronze_path)\n",
    "    if len(arquivos) == 0:\n",
    "        raise ValueError(f\"Nenhum arquivo encontrado em: {bronze_path}\")\n",
    "except Exception as e:\n",
    "    raise\n",
    "\n",
    "# >>> Leitura dos arquivos individualmente e união dos DataFrames\n",
    "    # - Verifica se o arquivo é do tipo .parquet.\n",
    "    # - Lê o arquivo com spark.read.parquet().  \n",
    "\n",
    "dfs = []\n",
    "\n",
    "for arquivo in arquivos:\n",
    "    if arquivo.path.endswith(\".parquet\"):\n",
    "        try:\n",
    "            df_temp = spark.read.parquet(arquivo.path)\n",
    "\n",
    "            # Forçar as colunas a terem o mesmo tipo (DoubleType) com .cast()\n",
    "            colunas_double = [\n",
    "                \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\",\n",
    "                \"tolls_amount\", \"improvement_surcharge\",\n",
    "                \"total_amount\", \"congestion_surcharge\",\n",
    "                \"trip_distance\"\n",
    "            ]\n",
    "\n",
    "            for coluna in colunas_double:\n",
    "                if coluna in df_temp.columns:\n",
    "                    df_temp = df_temp.withColumn(coluna, df_temp[coluna].cast(DoubleType()))\n",
    "\n",
    "            dfs.append(df_temp)\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError(\"Nenhum DataFrame carregado com sucesso.\")\n",
    "\n",
    "# União dos DataFrames\n",
    "# -Se nenhum DataFrame foi carregado com sucesso, gera um erro.\n",
    "#  -Caso contrário, une todos os DataFrames usando unionByName():\n",
    "df_final = dfs[0]\n",
    "for df_temp in dfs[1:]:\n",
    "    df_final = df_final.unionByName(df_temp, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96fb6855-7a80-462f-8e39-25672da0b82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Adiciona coluna com timestamp\n",
    "df_final = df_final.withColumn(\"date_processing\", F.current_timestamp())\n",
    "\n",
    "# Grava na tabela Delta\n",
    "df_final.write.format(\"delta\").mode(\"append\").saveAsTable(\"lakehouse.bronze.yellow_trip\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_data_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
